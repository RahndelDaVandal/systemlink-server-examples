{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src='SystemLink_icon.png' /></td>\n",
    "        <td ><h1><strong>NI SystemLink Python API</strong></h1></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Test Monitor Service Example\n",
    "***\n",
    "The Test Monitor Service API provides functions to create, update, delete and query Test results and Test steps.\n",
    "***\n",
    "# Prerequisites\n",
    "- The **NI SystemLink  Server Test Module** needs to be installed in order to run this example\n",
    "- The **NI SystemLink Client** needs to be installed on a system which has TestStand installed and is registered to the SystemLink server.  Configure the SystemLink TestStand plugin reporting to enable publishing test results.\n",
    "- Before you run this example, TestStand mock test results are needed:\n",
    "    - From **TestStand** open the **'Computer Motherboard Test Sequence.seq'**:\n",
    "        - Go to Help -> Find Examples and follow the instructions to open the Examples workspace (Examples.tsw)\n",
    "        - From the Workspace tab, expand **Demos** and select **Computer Motherboard Test**. Open one of the sequence files, based on your language of choice\n",
    "    - Run the sequence at least 10 times\n",
    "        - Make sure you fail several tests, on different components\n",
    "\n",
    "# Summary \n",
    "This notebook uses the Test Monitor Service API to import test and step results into Python. The data is used to do custom analytics.\n",
    "\n",
    "- Get all the test results that were created from the 'Computer Motherboard Test Sequence.seq' \n",
    "- Create a Pandas Dataframe with the information we want to process for each test\n",
    "- Plot pass vs. fail tests\n",
    "- Visualize test run vs. test duration\n",
    "- Pareto graph (step type)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from systemlink.testmonclient import TestMonitorClient, testmon_messages\n",
    "\n",
    "testmonclient = TestMonitorClient(service_name='TestMonitorClient')\n",
    "\n",
    "# Create pandas dataframe with the relevant test results information, to be used later\n",
    "def get_dataframe_from_results(results):\n",
    "    return pd.concat([pd.DataFrame({'status': result.status.status_name,\n",
    "                                    'startedAt': result.started_at,\n",
    "                                    'updatedAt': result.updated_at,\n",
    "                                    'programName': result.program_name,\n",
    "                                    'id': result.id,\n",
    "                                    'systemId': result.system_id,\n",
    "                                    'operator': result.operator,\n",
    "                                    'serialNumber': result.serial_number,\n",
    "                                    'totalTimeInSeconds': result.total_time_in_seconds,\n",
    "                                    }, index=[idx]) for idx, result in enumerate(results)])\n",
    "\n",
    "# Only query test results that belong to the 'Computer Motherboard Test Sequence.seq' test program\n",
    "query = testmon_messages.ResultQuery(program_names_=['Computer Motherboard Test Sequence.seq'])\n",
    "\n",
    "results, _ = testmonclient.query_results(query)\n",
    "df_results = get_dataframe_from_results(results)\n",
    "\n",
    "# Show the first elements of the dataframe, which holds the data we will use for further analysis\n",
    "df_results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar Plot of Test Results\n",
    "Group the tests results by pass/fail. Create a bar plot to visualize the test runs by result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tests results (pass/fail)\n",
    "\n",
    "bar_width = 0.4\n",
    "opacity = 0.4\n",
    "\n",
    "res = df_results.groupby('status').count()\n",
    "\n",
    "display(res)\n",
    "if 'Failed' not in res['id']:\n",
    "    raise RuntimeError('Please ensure that you execute at least one demo test that fails.')\n",
    "\n",
    "if 'Passed' not in res['id']:\n",
    "    raise RuntimeError('Please ensure that you execute at least one demo test that passes.')\n",
    "\n",
    "failed = res['id']['Failed']\n",
    "passed = res['id']['Passed']\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.bar(1, passed, bar_width, alpha=opacity, color='b', label='Pass')\n",
    "plt.bar(1.5, failed, bar_width, alpha=opacity, color='r', label='Fail')\n",
    "plt.xticks([1, 1.5], ['Pass', 'Fail'], size='15')\n",
    "plt.ylabel('Runs', size='15')\n",
    "plt.title('Total Runs: ' + str(passed + failed), weight='bold', size='15')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Test Run vs. Duration\n",
    "Visualize the test runs vs. duration, with red/green color indicating pass/fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test failures vs duration\n",
    "\n",
    "result_idx = np.arange(df_results.shape[0])\n",
    "\n",
    "df_time = df_results[['totalTimeInSeconds', 'status']]\n",
    "\n",
    "color = ['r' if status == 'Failed' else 'g' for status in df_time['status']]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "plt.scatter(result_idx, df_time['totalTimeInSeconds'], s=150, c=color, alpha='0.5')\n",
    "plt.title('Test Results - Duration', weight='bold', size='15')\n",
    "plt.xlabel('Test Runs', size='15')\n",
    "plt.ylabel('Time (seconds)', size='15')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto distribution\n",
    "Get a Pandas Dataframe with all the step failures. Visualize the failures in a Pareto graph, which helps visualize the failure distribution, by step type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto distribution of step failures visualization\n",
    "\n",
    "# Create pandas dataframe with the step results information that we want for further processing\n",
    "def get_failed_steps_dataframe(steps):\n",
    "    failed_steps = [step for step in steps if step.status.status_name == 'Failed' and step.step_type != 'SequenceCall']\n",
    "    if len(failed_steps) < 1:\n",
    "        raise RuntimeError('Please ensure that you execute at least one demo test that fails.')\n",
    "    return pd.concat([pd.DataFrame({'name': step.name,\n",
    "                                    'id': step.step_id,\n",
    "                                    'totalTimeInSeconds': step.total_time_in_seconds,\n",
    "                                    }, index=[idx]) for idx, step in enumerate(failed_steps)])\n",
    "\n",
    "result_ids = [result.id for result in results]\n",
    "step_query = testmon_messages.StepQuery(result_ids_=result_ids)\n",
    "\n",
    "steps, _ = testmonclient.query_steps(step_query)\n",
    "steps_df = get_failed_steps_dataframe(steps)\n",
    "res = steps_df.groupby('name').count()\n",
    "res = res.sort_values('id', ascending=False)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "plt.title('Failures by Test', weight='bold', size='15')\n",
    "plt.ylabel('Number of Runs', size='15')\n",
    "plt.xlabel('Test Type', size='15')\n",
    "ax1.get_xaxis().set_ticks([])\n",
    "\n",
    "# Create the Pareto chart bars\n",
    "previous_val = 0\n",
    "cumulative = []\n",
    "for idx, row in res.iterrows():\n",
    "    val = row['id']\n",
    "    cumulative.append(val + previous_val)\n",
    "    previous_val = val + previous_val\n",
    "    ax1.bar(idx, val, bar_width, alpha=opacity, label=idx)\n",
    "\n",
    "# Add a legend\n",
    "labels = list(steps_df['name'])\n",
    "plt.legend(labels, loc='upper right')\n",
    "\n",
    "# Cumulative line, in percentage\n",
    "cumulative_percentage = cumulative/cumulative[-1] * 100\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.plot(cumulative_percentage)\n",
    "plt.ylabel('Failure Percentage', size='15')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
